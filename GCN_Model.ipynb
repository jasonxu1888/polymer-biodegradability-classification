{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import copy\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "import openpyxl\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html for reference\n",
    "\n",
    "#two types of onehot encoding\n",
    "def onehot_encoding_unk(x, allowable_set):\n",
    "    #maps input not in the allowable set to the last element.\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "def onehot_encoding(x, allowable_set):\n",
    "    #raises exception if input not in allowable set\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set {1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "#returns 1D array of node (atom) features \n",
    "#function will be used later to return a matrix with shape of [num_atoms, num_atom_features] per compound/chemical\n",
    "atom_types = ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca','Fe','As','Al','I',\n",
    "              'B','V','K','Tl', 'Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn','H','Li',\n",
    "              'Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt','Hg','Pb','Unknown']\n",
    "atom_degrees = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "atom_implicit_valences = [0, 1, 2, 3, 4, 5, 6]\n",
    "atom_hybridizations = [Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "atom_num_Hs = [0, 1, 2, 3, 4]\n",
    "def atom_featurization(atom):\n",
    "    #featurization of each atom\n",
    "    atom_feats = np.array(onehot_encoding_unk(atom.GetSymbol(), atom_types) \n",
    "    + onehot_encoding(atom.GetDegree(), atom_degrees) \n",
    "    + onehot_encoding_unk(atom.GetImplicitValence(), atom_implicit_valences)\n",
    "    + [atom.GetFormalCharge()] \n",
    "    + [atom.GetNumRadicalElectrons()] \n",
    "    + onehot_encoding_unk(atom.GetHybridization(), atom_hybridizations) \n",
    "    + [atom.GetIsAromatic()] \n",
    "    + onehot_encoding_unk(atom.GetTotalNumHs(), atom_num_Hs))\n",
    "    return atom_feats\n",
    "\n",
    "\n",
    "#returns 1D array of edge (bond) features\n",
    "#function will be used later to return matrix of edge (bond) features with shape of [num_bonds, num_bond_features]\n",
    "bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "def bond_featurization(bond):\n",
    "    #featurization of each bond\n",
    "    bond_type = bond.GetBondType()\n",
    "    bond_feats = np.array([bond_type == Chem.rdchem.BondType.SINGLE, bond_type == Chem.rdchem.BondType.DOUBLE, bond_type == Chem.rdchem.BondType.TRIPLE, bond_type == Chem.rdchem.BondType.AROMATIC] \n",
    "    + [bond.GetIsConjugated()] \n",
    "    + [bond.IsInRing()])\n",
    "    return bond_feats\n",
    "\n",
    "\n",
    "#returns list of index tuples (bond indexes) that accounts for both directions of each bond\n",
    "def bond_pairs_between_atoms(mol):                                   \n",
    "    bonds = mol.GetBonds()\n",
    "    bond_index_tuples = []\n",
    "    for bond in bonds:\n",
    "        bond_index_tuples.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]) #forward direction e.g. [0,1]\n",
    "        bond_index_tuples.append([bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]) #backward direction [1,0]\n",
    "    return bond_index_tuples\n",
    "\n",
    "\n",
    "#returns a pyg Data object from MOL objects using the functions above\n",
    "def mol_to_graph(mol):\n",
    "    atoms = mol.GetAtoms()\n",
    "    bonds = mol.GetBonds()\n",
    "    node_features= [atom_featurization(atom) for atom in atoms]\n",
    "\n",
    "    #Applies bond_featurization function twice to make it the same dimensions as the edge_index matrix since both directions of the bond is accounted for\n",
    "    edge_features = [bond_featurization(bond) for bond in bonds]\n",
    "    for bond in bonds:\n",
    "        edge_features.append(bond_featurization(bond))\n",
    "    #\n",
    "    \n",
    "    edge_index = bond_pairs_between_atoms(mol)\n",
    "    #pyg Data object construction\n",
    "    data = Data(x=torch.tensor(node_features, dtype=torch.float),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous(),\n",
    "                edge_attr=torch.tensor(edge_features, dtype=torch.float))\n",
    "    return data\n",
    "\n",
    "#returns dict with mol object as the key and the value being the class (nonreadibly biodegradeable VS readibly biodegradeable).\n",
    "def MOL_class_dict_from_SMILES(dataframe):\n",
    "    mols_class_dict = {}\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        mols_class_dict[Chem.MolFromSmiles(dataframe['SMILES'].iloc[i])] = dataframe['Class'].iloc[i]\n",
    "    return mols_class_dict\n",
    "\n",
    "#returns list of all pyg Data objects for each SMILES/MOL\n",
    "#Each Data object contains the graph information (node features, edge features, and edge index) as well as the ground truth/classification (NRB vs RB)\n",
    "def make_all_graphs(mols_class_dict):\n",
    "    X = [mol_to_graph(m) for m in mols_class_dict.keys()]\n",
    "    #Adds the classification/ground truth field, object_variable_name.y, for each pyg Data object\n",
    "    for i, dataobject in enumerate(X):\n",
    "        dataobject.y = torch.tensor([list(mols_class_dict.values())[i]], dtype=torch.long) \n",
    "    #\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing architecture of the model: 4-layer GCN and 3-layer MLP combined\n",
    "class GCNlayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, conv_dim1, conv_dim2, conv_dim3, concat_dim, dropout):\n",
    "        super(GCNlayer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.conv_dim1 = conv_dim1\n",
    "        self.conv_dim2 = conv_dim2\n",
    "        self.conv_dim3 = conv_dim3\n",
    "        self.concat_dim =  concat_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.conv1 = GCNConv(self.n_features, self.conv_dim1)\n",
    "        self.bn1 = BatchNorm1d(self.conv_dim1)\n",
    "        self.conv2 = GCNConv(self.conv_dim1, self.conv_dim2)\n",
    "        self.bn2 = BatchNorm1d(self.conv_dim2)\n",
    "        self.conv3 = GCNConv(self.conv_dim2, self.conv_dim3)\n",
    "        self.bn3 = BatchNorm1d(self.conv_dim3)\n",
    "        self.conv4 = GCNConv(self.conv_dim3, self.concat_dim)\n",
    "        self.bn4 = BatchNorm1d(self.concat_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "    \n",
    "class FClayer(nn.Module): \n",
    "    \n",
    "    def __init__(self, concat_dim, pred_dim1, pred_dim2, out_dim, dropout):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.concat_dim = concat_dim\n",
    "        self.pred_dim1 = pred_dim1\n",
    "        self.pred_dim2 = pred_dim2\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.fc1 = Linear(self.concat_dim, self.pred_dim1)\n",
    "        self.bn1 = BatchNorm1d(self.pred_dim1)\n",
    "        self.fc2 = Linear(self.pred_dim1, self.pred_dim2)\n",
    "        self.fc3 = Linear(self.pred_dim2, self.out_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = F.relu(self.fc1(data))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = GCNlayer(args.n_features, \n",
    "                              args.conv_dim1, \n",
    "                              args.conv_dim2, \n",
    "                              args.conv_dim3, \n",
    "                              args.concat_dim, \n",
    "                              args.dropout)\n",
    "\n",
    "        self.fc = FClayer(args.concat_dim, \n",
    "                          args.pred_dim1, \n",
    "                          args.pred_dim2, \n",
    "                          args.out_dim, \n",
    "                          args.dropout) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = self.conv(data)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model state for transfer learning later, called in experiment function below\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, filename):\n",
    "    state = {\n",
    "        'Epoch': epoch,\n",
    "        'State_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, filename)\n",
    "\n",
    "#training model on training set, called in experiment function below\n",
    "def train(model, device, optimizer, train_loader, criterion, args):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    epoch_train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = data.y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        outputs.require_grad = False\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_acc =  100 * train_correct / train_total\n",
    "    return model, epoch_train_loss, train_acc\n",
    "\n",
    "#evaluating performance metrics of trained model on the test set, called in experiment function below\n",
    "def test(model, device, test_loader, args): \n",
    "    model.eval()\n",
    "    classes = ('RB', 'NRB')\n",
    "    class_correct = list(0. for i in range(len(classes)))\n",
    "    class_total = list(0. for i in range(len(classes)))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    nb_classes = len(classes)\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    test_hist = {\"test_acc\":[]}\n",
    "    y_score =[]\n",
    "    y_test =[]\n",
    "    data_total = []\n",
    "    pred_data_total = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            labels = data.y.to(device)\n",
    "            data_total += data.y.tolist()\n",
    "            outputs = model(data)\n",
    "            pred_data_total += outputs.view(-1).tolist()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            y_score.append(outputs.cpu().numpy()) \n",
    "            y_test.append(labels.cpu().numpy()) \n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "            for i in range(labels.shape[0]):\n",
    "                if(labels.shape[0] == 1):\n",
    "                    label = labels[i]\n",
    "                    class_correct[label] += c.item()\n",
    "                    class_total[label] += 1\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    test_hist[\"test_acc\"].append((predicted == labels).sum().item())\n",
    "                else:  \n",
    "                    label = labels[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    test_hist[\"test_acc\"].append((predicted == labels).sum().item())\n",
    "\n",
    "    data_total = list(labels.view(-1).cpu().numpy()) #true class\n",
    "    pred_data_total = list(predicted.view(-1).cpu().numpy()) #model predicted class, create cm using this\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_score][0]\n",
    "    conf = confusion_matrix.tolist()\n",
    "    total_acc = 100 * correct / total\n",
    "    rb = -1\n",
    "    nrb = -1\n",
    "    if class_total[0] != 0:\n",
    "        rb = 100 * class_correct[0] / class_total[0]\n",
    "    if class_total[1] != 0:\n",
    "        nrb = 100 * class_correct[1] / class_total[1]\n",
    "    miscore = f1_score(data_total, pred_data_total, average='micro')\n",
    "    mascore = f1_score(data_total, pred_data_total, average='macro')\n",
    "    ba = balanced_accuracy_score(data_total, pred_data_total)\n",
    "    er = (confusion_matrix[0,1]+confusion_matrix[1,0])/sum(sum(confusion_matrix))\n",
    "    return conf, total_acc, rb, nrb, miscore, mascore, data_total, pred_data_total, y_pred_list, ba, er\n",
    "\n",
    "# Overall function that takes the defined model architecture, training set, testing set, device to use, and hyperparameters. Utilizes both functions above.\n",
    "def experiment(model, train_loader, test_loader, device, args, trainmodel = True, savemodel = True): \n",
    "    time_start = time.time()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=args.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step_size,\n",
    "                                          gamma=args.gamma)\n",
    "    #training here\n",
    "    list_train_loss = list()\n",
    "    list_train_acc = list()\n",
    "    if trainmodel:\n",
    "        #print('[Train]')\n",
    "        for epoch in range(args.epoch):\n",
    "            scheduler.step()\n",
    "            #print('- Epoch :', epoch+1)\n",
    "            model, train_loss, train_acc = train(model, device, optimizer, train_loader, criterion, args)\n",
    "            list_train_loss.append(train_loss)\n",
    "            list_train_acc.append(train_acc)\n",
    "            #print('Loss : %.4f' % list_train_loss[-1], '| Accuracy : %.4f' % list_train_acc[-1])\n",
    "    \n",
    "    #testing here\n",
    "    conf, total_acc, RB, NRB, miscore, mascore, data_total, pred_data_total, y_pred_list, ba, er = test(model, device, test_loader, args)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_train_acc = list_train_acc\n",
    "    args.data_total = data_total\n",
    "    args.pred_data_total = pred_data_total\n",
    "    args.conf = conf\n",
    "    args.total_acc = total_acc\n",
    "    args.RB = RB\n",
    "    args.NRB = NRB\n",
    "    args.miscore = miscore\n",
    "    args.mascore = mascore\n",
    "    args.time_required = time_required\n",
    "    args.y_pred_list = y_pred_list\n",
    "    args.ba = ba\n",
    "    args.er = er\n",
    "    \n",
    "    #saving model state here\n",
    "    if savemodel:\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, 'model.pt')\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN_BA</th>\n",
       "      <th>GCN_Sn</th>\n",
       "      <th>GCN_Sp</th>\n",
       "      <th>GCN_ER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.852125</td>\n",
       "      <td>0.83105</td>\n",
       "      <td>0.873199</td>\n",
       "      <td>0.14311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GCN_BA   GCN_Sn    GCN_Sp   GCN_ER\n",
       "0  0.852125  0.83105  0.873199  0.14311"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model application on original aggreated dataset (~3k)\n",
    "args.epoch = 400\n",
    "args.lr = 0.0001\n",
    "args.optim = 'Adam'\n",
    "args.step_size = 10\n",
    "args.gamma = 0.9\n",
    "args.dropout = 0.1\n",
    "args.n_features = 75\n",
    "dim = 512\n",
    "args.conv_dim1 = dim\n",
    "args.conv_dim2 = dim\n",
    "args.conv_dim3 = dim\n",
    "args.concat_dim = dim\n",
    "args.pred_dim1 = dim\n",
    "args.pred_dim2 = dim\n",
    "args.out_dim = 2\n",
    "\n",
    "BA = []\n",
    "Sn = []\n",
    "Sp = []\n",
    "ER = []\n",
    "\n",
    "t=0.2\n",
    "r=42\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "df = pd.read_csv('C:/Users/trifo/Argonne SULI Project/src/Datasets\\RB_vs_NRB_Compounds_(Tentative).csv', low_memory=False)\n",
    "df = pd.concat([df['SMILES'], df['Class']], axis=1)\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=t, shuffle=True, stratify=df['Class'], random_state=r)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "train_mols = MOL_class_dict_from_SMILES(X_train)\n",
    "test_mols = MOL_class_dict_from_SMILES(X_test)\n",
    "\n",
    "train_X = make_all_graphs(train_mols)\n",
    "test_X = make_all_graphs(test_mols)\n",
    "\n",
    "model = Net(args)\n",
    "model = model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_X, batch_size=len(train_X), shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_X, batch_size=len(test_X), shuffle=False, drop_last=True)\n",
    "\n",
    "dict_result = dict()\n",
    "args.exp_name = 'Test'\n",
    "result = vars(experiment(model, train_loader, test_loader, device, args, trainmodel = True, savemodel = True))\n",
    "dict_result[args.exp_name] = copy.deepcopy(result)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "res_df = pd.DataFrame(dict_result).transpose()\n",
    "c = res_df['conf'].iloc[0]\n",
    "confusion_matrix = np.array([c[0], c[1]], dtype=float)\n",
    "cm = confusion_matrix\n",
    "sensitivity1 = cm[1,1]/(cm[1,1]+cm[1,0])\n",
    "specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "er = (cm[0,1]+cm[1,0])/sum(sum(cm))\n",
    "BA.append(res_df['ba'].iloc[0])\n",
    "Sn.append(sensitivity1)\n",
    "Sp.append(specificity1)\n",
    "ER.append(er)\n",
    "results_df = pd.DataFrame({'GCN_BA':BA, 'GCN_Sn':Sn, 'GCN_Sp':Sp, 'GCN_ER':ER})\n",
    "results_df.to_csv('GCN_metrics.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2.]\n",
      " [0. 4.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN_tl_BA</th>\n",
       "      <th>GCN_tl_Sn</th>\n",
       "      <th>GCN_tl_Sp</th>\n",
       "      <th>GCN_tl_ER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GCN_tl_BA  GCN_tl_Sn  GCN_tl_Sp  GCN_tl_ER\n",
       "0        0.5        1.0        0.0   0.333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testing perfomance of model on new domain/dataset of polymers, NO TRAINING done here\n",
    "#USING FRESHWATER DATASET\n",
    "tl_BA = []\n",
    "tl_Sn = []\n",
    "tl_Sp = []\n",
    "tl_ER = []\n",
    "\n",
    "tl_df = pd.read_excel('C:/Users/trifo/Argonne SULI Project/src/GCN_RB_NRB_Model/TL_Freshwater__Dataset.xlsx')\n",
    "tl_test = pd.concat([tl_df['SMILES'], tl_df['Class']], axis=1)\n",
    "tl_test_mols = MOL_class_dict_from_SMILES(tl_test)\n",
    "tl_test_X = make_all_graphs(tl_test_mols)\n",
    "\n",
    "tl_train_loader = DataLoader([])\n",
    "tl_test_loader = DataLoader(tl_test_X, batch_size=len(tl_test_X), shuffle=False, drop_last = True)\n",
    "\n",
    "tl_dict_result = dict()\n",
    "args.exp_name = 'tl_test'\n",
    "tl_result = vars(experiment(model, tl_train_loader, tl_test_loader, device, args, trainmodel = False, savemodel = False))\n",
    "tl_dict_result[args.exp_name] = copy.deepcopy(tl_result)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tl_res_df = pd.DataFrame(tl_dict_result).transpose()\n",
    "tl_conf = tl_res_df['conf'].iloc[0]\n",
    "tl_confusion_matrix = np.array([tl_conf[0], tl_conf[1]], dtype=float)\n",
    "tlcm = tl_confusion_matrix\n",
    "tl_sensitivity1 = tlcm[1,1]/(tlcm[1,1]+tlcm[1,0])\n",
    "tl_specificity1 = tlcm[0,0]/(tlcm[0,0]+tlcm[0,1])\n",
    "tl_er = (tlcm[0,1]+tlcm[1,0])/sum(sum(tlcm))\n",
    "tl_BA.append(tl_res_df['ba'].iloc[0])\n",
    "tl_Sn.append(tl_sensitivity1)\n",
    "tl_Sp.append(tl_specificity1)\n",
    "tl_ER.append(tl_er)\n",
    "\n",
    "tl_results_df = pd.DataFrame({'GCN_tl_BA':tl_BA, 'GCN_tl_Sn':tl_Sn, 'GCN_tl_Sp':tl_Sp, 'GCN_tl_ER':tl_ER})\n",
    "tl_results_df.to_csv('GCN_tl_metrics.csv', index=False)\n",
    "print(tlcm)\n",
    "display(tl_results_df)\n",
    "#without training model predicts all as RB (1), so it predicts 2 incorrectly since PBS and PBAdip are NRB (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 9.]\n",
      " [0. 6.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN_tl_BA</th>\n",
       "      <th>GCN_tl_Sn</th>\n",
       "      <th>GCN_tl_Sp</th>\n",
       "      <th>GCN_tl_ER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GCN_tl_BA  GCN_tl_Sn  GCN_tl_Sp  GCN_tl_ER\n",
       "0        0.5        1.0        0.0        0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testing perfomance of model on new domain/dataset of polymers, NO TRAINING done here\n",
    "#USING SEAWATER(Marine) Dataset\n",
    "tl_BA = []\n",
    "tl_Sn = []\n",
    "tl_Sp = []\n",
    "tl_ER = []\n",
    "\n",
    "tl_df = pd.read_excel('C:/Users/trifo/Argonne SULI Project/src/GCN_RB_NRB_Model/TL_Seawater(Marine)_Dataset.xlsx')\n",
    "tl_test = pd.concat([tl_df['SMILES'], tl_df['Class']], axis=1)\n",
    "tl_test_mols = MOL_class_dict_from_SMILES(tl_test)\n",
    "tl_test_X = make_all_graphs(tl_test_mols)\n",
    "\n",
    "tl_train_loader = DataLoader([])\n",
    "tl_test_loader = DataLoader(tl_test_X, batch_size=len(tl_test_X), shuffle=False, drop_last = True)\n",
    "\n",
    "tl_dict_result = dict()\n",
    "args.exp_name = 'tl_test'\n",
    "tl_result = vars(experiment(model, tl_train_loader, tl_test_loader, device, args, trainmodel = False, savemodel = False))\n",
    "tl_dict_result[args.exp_name] = copy.deepcopy(tl_result)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tl_res_df = pd.DataFrame(tl_dict_result).transpose()\n",
    "tl_conf = tl_res_df['conf'].iloc[0]\n",
    "tl_confusion_matrix = np.array([tl_conf[0], tl_conf[1]], dtype=float)\n",
    "tlcm = tl_confusion_matrix\n",
    "tl_sensitivity1 = tlcm[1,1]/(tlcm[1,1]+tlcm[1,0])\n",
    "tl_specificity1 = tlcm[0,0]/(tlcm[0,0]+tlcm[0,1])\n",
    "tl_er = (tlcm[0,1]+tlcm[1,0])/sum(sum(tlcm))\n",
    "tl_BA.append(tl_res_df['ba'].iloc[0])\n",
    "tl_Sn.append(tl_sensitivity1)\n",
    "tl_Sp.append(tl_specificity1)\n",
    "tl_ER.append(tl_er)\n",
    "\n",
    "tl_results_df = pd.DataFrame({'GCN_tl_BA':tl_BA, 'GCN_tl_Sn':tl_Sn, 'GCN_tl_Sp':tl_Sp, 'GCN_tl_ER':tl_ER})\n",
    "tl_results_df.to_csv('GCN_tl_metrics.csv', index=False)\n",
    "print(tlcm)\n",
    "display(tl_results_df)\n",
    "#predicting all of the values as 1, so 9 polymers that are 0 (NRB) are being predicted incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 ...\n",
      "Fold 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 ...\n",
      "[[2. 1.]\n",
      " [1. 2.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN_tl_BA</th>\n",
       "      <th>GCN_tl_Sn</th>\n",
       "      <th>GCN_tl_Sp</th>\n",
       "      <th>GCN_tl_ER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GCN_tl_BA  GCN_tl_Sn  GCN_tl_Sp  GCN_tl_ER\n",
       "0   0.666667   0.666667   0.666667   0.333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#transfer learning through LOO (leave one out) validation for FRESHWATER\n",
    "\n",
    "#altering experiment function for this portion\n",
    "def kf_experiment(model, optimizer, scheduler, train_loader, test_loader, device, args, trainmodel = True, savemodel = False): \n",
    "    time_start = time.time()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #training here\n",
    "    list_train_loss = list()\n",
    "    list_train_acc = list()\n",
    "    if trainmodel:\n",
    "        #print('[Train]')\n",
    "        for epoch in range(args.epoch):\n",
    "            scheduler.step()\n",
    "            #print('- Epoch :', epoch+1)\n",
    "            model, train_loss, train_acc = train(model, device, optimizer, train_loader, criterion, args)\n",
    "            list_train_loss.append(train_loss)\n",
    "            list_train_acc.append(train_acc)\n",
    "            #print('Loss : %.4f' % list_train_loss[-1], '| Accuracy : %.4f' % list_train_acc[-1])\n",
    "    \n",
    "    #testing here\n",
    "    conf, total_acc, RB, NRB, miscore, mascore, data_total, pred_data_total, y_pred_list, ba, er = test(model, device, test_loader, args)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_train_acc = list_train_acc\n",
    "    args.data_total = data_total\n",
    "    args.pred_data_total = pred_data_total\n",
    "    args.conf = conf\n",
    "    args.total_acc = total_acc\n",
    "    args.RB = RB\n",
    "    args.NRB = NRB\n",
    "    args.miscore = miscore\n",
    "    args.mascore = mascore\n",
    "    args.time_required = time_required\n",
    "    args.y_pred_list = y_pred_list\n",
    "    args.ba = ba\n",
    "    args.er = er\n",
    "    \n",
    "    #saving model state here\n",
    "    if savemodel:\n",
    "        save_checkpoint(epoch, model, optimizer, 'model.pt')\n",
    "\n",
    "    return args\n",
    "\n",
    "#ai for pfas code, transferlearning notebook, benchmarks bayesian gpyops\n",
    "\n",
    "####### Model application\n",
    "args.epoch = 1000\n",
    "args.lr = 0.0001\n",
    "args.optim = 'Adam'\n",
    "args.step_size = 10\n",
    "args.gamma = 0.9\n",
    "args.dropout = 0.1\n",
    "args.n_features = 75\n",
    "dim = 512\n",
    "args.conv_dim1 = dim\n",
    "args.conv_dim2 = dim\n",
    "args.conv_dim3 = dim\n",
    "args.concat_dim = dim\n",
    "args.pred_dim1 = dim\n",
    "args.pred_dim2 = dim\n",
    "args.out_dim = 2\n",
    "\n",
    "kf_df = pd.read_excel('C:/Users/trifo/Argonne SULI Project/src/GCN_RB_NRB_Model/TL_Freshwater__Dataset.xlsx')\n",
    "kf_df = pd.concat([kf_df['SMILES'], kf_df['Class']], axis=1)\n",
    "kfold = KFold(n_splits=len(kf_df))\n",
    "\n",
    "kf_BA = []\n",
    "kf_Sn = []\n",
    "kf_Sp = []\n",
    "kf_ER = []\n",
    "\n",
    "CM_list = []\n",
    "\n",
    "validation = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(kf_df)):\n",
    "    print('Fold', fold+1, '...')\n",
    "    kf_train = kf_df.iloc[train_index]\n",
    "    kf_test = kf_df.iloc[test_index]\n",
    "    kf_train = kf_train.reset_index(drop=True)\n",
    "    kf_test = kf_test.reset_index(drop=True)\n",
    "    \n",
    "    kf_train_mols = MOL_class_dict_from_SMILES(kf_train)\n",
    "    kf_test_mols = MOL_class_dict_from_SMILES(kf_test)\n",
    "\n",
    "    train_kf = make_all_graphs(kf_train_mols)\n",
    "    test_kf = make_all_graphs(kf_test_mols)\n",
    "\n",
    "    kf_train_loader = DataLoader(train_kf, batch_size=len(train_kf), shuffle=True, drop_last=True)\n",
    "    kf_test_loader = DataLoader(test_kf, batch_size=len(test_kf), shuffle=False, drop_last=True)\n",
    "\n",
    "    checkpoint = torch.load('model.pt')\n",
    "    kf_model = Net(args)\n",
    "    kf_model.load_state_dict(checkpoint['State_dict'])\n",
    "    kf_model = kf_model.to(device)\n",
    "\n",
    "    kf_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    kf_optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    kf_scheduler = optim.lr_scheduler.StepLR(kf_optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "    kf_scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    #freezing layers/parameters, play around with this\n",
    "    count = 0\n",
    "    for param in kf_model.parameters():\n",
    "        count += 1\n",
    "        if count < 15:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    kf_dict_result = dict()\n",
    "    args.exp_name = 'kf_test'\n",
    "    kf_result = vars(kf_experiment(kf_model, kf_optimizer, kf_scheduler, kf_train_loader, kf_test_loader, device, args, trainmodel = True, savemodel = False))\n",
    "    kf_dict_result[args.exp_name] = copy.deepcopy(kf_result)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    kf_res_df = pd.DataFrame(kf_dict_result).transpose()\n",
    "    kf_conf = kf_res_df['conf'].iloc[0]\n",
    "    kf_confusion_matrix = np.array([kf_conf[0], kf_conf[1]], dtype=float)\n",
    "\n",
    "\n",
    "    CM_list.append(kf_confusion_matrix)\n",
    "\n",
    "CM_combined = np.zeros((2,2))\n",
    "for cm in CM_list:\n",
    "    CM_combined[0,0] += cm[0,0]\n",
    "    CM_combined[0,1] += cm[0,1]\n",
    "    CM_combined[1,0] += cm[1,0]\n",
    "    CM_combined[1,1] += cm[1,1]\n",
    "\n",
    "kfcm = CM_combined\n",
    "kf_sensitivity1 = kfcm[1,1]/(kfcm[1,1]+kfcm[1,0])\n",
    "kf_specificity1 = kfcm[0,0]/(kfcm[0,0]+kfcm[0,1])\n",
    "kf_er = (kfcm[0,1]+kfcm[1,0])/sum(sum(kfcm))\n",
    "kf_Sn.append(kf_sensitivity1)\n",
    "kf_Sp.append(kf_specificity1)\n",
    "kf_BA.append((kf_sensitivity1 + kf_specificity1)/2)\n",
    "kf_ER.append(kf_er)\n",
    "\n",
    "kf_results_df = pd.DataFrame({'GCN_tl_BA':kf_BA, 'GCN_tl_Sn':kf_Sn, 'GCN_tl_Sp':kf_Sp, 'GCN_tl_ER':kf_ER})\n",
    "kf_results_df.to_csv('GCN_kf_metrics.csv', index=False)\n",
    "print(kfcm)\n",
    "display(kf_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 ...\n",
      "Fold 8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 12 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 13 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 14 ...\n",
      "Fold 15 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trifo\\anaconda3\\envs\\GCNenv\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 4.]\n",
      " [5. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN_tl_BA</th>\n",
       "      <th>GCN_tl_Sn</th>\n",
       "      <th>GCN_tl_Sp</th>\n",
       "      <th>GCN_tl_ER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GCN_tl_BA  GCN_tl_Sn  GCN_tl_Sp  GCN_tl_ER\n",
       "0   0.361111   0.166667   0.555556        0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#transfer learning through LOO (leave one out) validation for MARINE\n",
    "\n",
    "#altering experiment function for this portion\n",
    "def kf_experiment(model, optimizer, scheduler, train_loader, test_loader, device, args, trainmodel = True, savemodel = False): \n",
    "    time_start = time.time()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #training here\n",
    "    list_train_loss = list()\n",
    "    list_train_acc = list()\n",
    "    if trainmodel:\n",
    "        #print('[Train]')\n",
    "        for epoch in range(args.epoch):\n",
    "            scheduler.step()\n",
    "            #print('- Epoch :', epoch+1)\n",
    "            model, train_loss, train_acc = train(model, device, optimizer, train_loader, criterion, args)\n",
    "            list_train_loss.append(train_loss)\n",
    "            list_train_acc.append(train_acc)\n",
    "            #print('Loss : %.4f' % list_train_loss[-1], '| Accuracy : %.4f' % list_train_acc[-1])\n",
    "    \n",
    "    #testing here\n",
    "    conf, total_acc, RB, NRB, miscore, mascore, data_total, pred_data_total, y_pred_list, ba, er = test(model, device, test_loader, args)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_train_acc = list_train_acc\n",
    "    args.data_total = data_total\n",
    "    args.pred_data_total = pred_data_total\n",
    "    args.conf = conf\n",
    "    args.total_acc = total_acc\n",
    "    args.RB = RB\n",
    "    args.NRB = NRB\n",
    "    args.miscore = miscore\n",
    "    args.mascore = mascore\n",
    "    args.time_required = time_required\n",
    "    args.y_pred_list = y_pred_list\n",
    "    args.ba = ba\n",
    "    args.er = er\n",
    "    \n",
    "    #saving model state here\n",
    "    if savemodel:\n",
    "        save_checkpoint(epoch, model, optimizer, 'model.pt')\n",
    "\n",
    "    return args\n",
    "\n",
    "#ai for pfas code, transferlearning notebook, benchmarks bayesian gpyops\n",
    "\n",
    "####### Model application\n",
    "args.epoch = 5000\n",
    "args.lr = 0.0001\n",
    "args.optim = 'Adam'\n",
    "args.step_size = 10\n",
    "args.gamma = 0.9\n",
    "args.dropout = 0.1\n",
    "args.n_features = 75\n",
    "dim = 512\n",
    "args.conv_dim1 = dim\n",
    "args.conv_dim2 = dim\n",
    "args.conv_dim3 = dim\n",
    "args.concat_dim = dim\n",
    "args.pred_dim1 = dim\n",
    "args.pred_dim2 = dim\n",
    "args.out_dim = 2\n",
    "\n",
    "kf_df = pd.read_excel('C:/Users/trifo/Argonne SULI Project/src/GCN_RB_NRB_Model/TL_Seawater(Marine)_Dataset.xlsx')\n",
    "kf_df = pd.concat([kf_df['SMILES'], kf_df['Class']], axis=1)\n",
    "kfold = KFold(n_splits=len(kf_df))\n",
    "\n",
    "kf_BA = []\n",
    "kf_Sn = []\n",
    "kf_Sp = []\n",
    "kf_ER = []\n",
    "\n",
    "CM_list = []\n",
    "\n",
    "validation = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(kf_df)):\n",
    "    print('Fold', fold+1, '...')\n",
    "    kf_train = kf_df.iloc[train_index]\n",
    "    kf_test = kf_df.iloc[test_index]\n",
    "    kf_train = kf_train.reset_index(drop=True)\n",
    "    kf_test = kf_test.reset_index(drop=True)\n",
    "    \n",
    "    kf_train_mols = MOL_class_dict_from_SMILES(kf_train)\n",
    "    kf_test_mols = MOL_class_dict_from_SMILES(kf_test)\n",
    "\n",
    "    train_kf = make_all_graphs(kf_train_mols)\n",
    "    test_kf = make_all_graphs(kf_test_mols)\n",
    "\n",
    "    kf_train_loader = DataLoader(train_kf, batch_size=len(train_kf), shuffle=True, drop_last=True)\n",
    "    kf_test_loader = DataLoader(test_kf, batch_size=len(test_kf), shuffle=False, drop_last=True)\n",
    "\n",
    "    checkpoint = torch.load('model.pt')\n",
    "    kf_model = Net(args)\n",
    "    kf_model.load_state_dict(checkpoint['State_dict'])\n",
    "    kf_model = kf_model.to(device)\n",
    "\n",
    "    kf_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    kf_optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    kf_scheduler = optim.lr_scheduler.StepLR(kf_optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "    kf_scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    #freezing layers/parameters, play around with this\n",
    "    count = 0\n",
    "    for param in kf_model.parameters():\n",
    "        count += 1\n",
    "        if count < 23:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    kf_dict_result = dict()\n",
    "    args.exp_name = 'kf_test'\n",
    "    kf_result = vars(kf_experiment(kf_model, kf_optimizer, kf_scheduler, kf_train_loader, kf_test_loader, device, args, trainmodel = True, savemodel = False))\n",
    "    kf_dict_result[args.exp_name] = copy.deepcopy(kf_result)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    kf_res_df = pd.DataFrame(kf_dict_result).transpose()\n",
    "    kf_conf = kf_res_df['conf'].iloc[0]\n",
    "    kf_confusion_matrix = np.array([kf_conf[0], kf_conf[1]], dtype=float)\n",
    "\n",
    "\n",
    "    CM_list.append(kf_confusion_matrix)\n",
    "\n",
    "CM_combined = np.zeros((2,2))\n",
    "for cm in CM_list:\n",
    "    CM_combined[0,0] += cm[0,0]\n",
    "    CM_combined[0,1] += cm[0,1]\n",
    "    CM_combined[1,0] += cm[1,0]\n",
    "    CM_combined[1,1] += cm[1,1]\n",
    "\n",
    "kfcm = CM_combined\n",
    "kf_sensitivity1 = kfcm[1,1]/(kfcm[1,1]+kfcm[1,0])\n",
    "kf_specificity1 = kfcm[0,0]/(kfcm[0,0]+kfcm[0,1])\n",
    "kf_er = (kfcm[0,1]+kfcm[1,0])/sum(sum(kfcm))\n",
    "kf_Sn.append(kf_sensitivity1)\n",
    "kf_Sp.append(kf_specificity1)\n",
    "kf_BA.append((kf_sensitivity1 + kf_specificity1)/2)\n",
    "kf_ER.append(kf_er)\n",
    "\n",
    "kf_results_df = pd.DataFrame({'GCN_tl_BA':kf_BA, 'GCN_tl_Sn':kf_Sn, 'GCN_tl_Sp':kf_Sp, 'GCN_tl_ER':kf_ER})\n",
    "kf_results_df.to_csv('GCN_kf_metrics.csv', index=False)\n",
    "print(kfcm)\n",
    "display(kf_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('GCNenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd7aa03f636f56e22c7abe68884ee9c57f7a62bb34bcc2920803c4a7036015f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
